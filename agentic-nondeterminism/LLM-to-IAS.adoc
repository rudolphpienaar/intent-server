= Reasoning, LLMs, and Agentic Programs: Agents Will Always Lie, So Clinical Truth Must Live Outside the Model
Author: Rudolph Pienaar
:doctype: article
:stem:

== Abstract

This article can be viewed as an attempt to explore the consequences of three propositions. First, modern AI systems rest on neural networks that, in their classical predictive role, are deterministic: for fixed parameters and an input, the forward pass always produces the same output. These networks act as compressive maps from high-dimensional inputs onto lower-dimensional manifolds of internal representations. Second, generative AI uses essentially the same machinery in reverse spirit. Instead of compressing data into embeddings and predicting labels, we ask models to produce text, images, or code from context. Mathematically, this reverse direction cannot be a true inverse, because the forward map is many-to-one and not invertible; there is no single input to recover. The only meaningful way back is probabilistic, by learning a conditional distribution over the many inputs compatible with a representation and sampling from it. Probability is therefore not an optional detail on top of otherwise deterministic networks; it is intrinsic to the generative step. Third, in many current proposals this inherently probabilistic generator is placed in charge of assembling workflows, selecting tools, and orchestrating multi-step procedures, including in clinical settings, often without an explicit appreciation of the mathematical limits that follow.

From these propositions follow two claims. The first is that once a probabilistic policy is responsible for exploring a large space of possible action sequences that contains both valid and invalid workflows, the probability of selecting an invalid workflow cannot be driven to zero without removing the agent’s freedom to assemble workflows. I make this precise by modeling tools as nodes in a directed graph, workflows as paths, and an agent as a probability distribution over those paths. Under mild assumptions, any non-trivial agentic policy over this graph has non-zero mass on clinically invalid paths; as workflows grow longer and tools more numerous, the probability of at least one unacceptable orchestration choice increases rather than decreases. In this sense, such systems will always “lie” in the sense that matters in clinical contexts: they will inevitably produce procedurally incorrect or unsafe workflows with non-zero probability. Over roughly the last year, however, vendors and research groups have begun to promote increasingly agentic systems for healthcare and other high-stakes domains, often presenting tool-using agents as a natural next step beyond traditional programming and clinical decision support. Many scientists and engineers who are not steeped in the details of generative modeling accept these proposals at face value or propose agentic systems of their own. The growing enthusiasm for ever more agentic systems is therefore not just philosophically troubling; it is a concrete safety concern to the extent that it ignores this structural constraint.

The second claim is architectural. If one accepts this mathematical constraint, the way to avoid orchestration hallucination is not to demand more elaborate reasoning traces from the agent, but to take generative models out of the orchestration loop. Execution must be restricted to a set of prevalidated, deterministic workflows, and large language models must be used only as translators from human needs and clinical narratives to formal intents that select among those workflows. I describe an Intent–Action Service that compiles each intent to a single canonical workflow, collapsing the combinatorial action space to one path per intent and localizing residual uncertainty to the interpretation of user intent rather than to the construction of procedures. I argue that for clinical and other safety-critical domains, this separation between probabilistic interpretation and deterministic execution is not merely advisable but required. +

== Introduction

"Lies, damned lies, and statistics"footnote:[Often attributed to Mark Twain, possibly originally Benjamin Disraeli.] is a reminder that numbers can be made to serve any story. If statistics can bend, so can its sibling: probability. It lets us say, “the truth is out there, somewhere in this cloud,” when pinning a single point is hard or impossible. Probability was supposed to help us grapple with complex, subsampled phenomena. A 95% chance of rain tomorrow was never meant to mean it _will_ rain. Many seem to forget that probability is the engine in every computational atom of generative AI, dazzling with an appearance of determinism. Sometimes that “good enough” shine hides unacceptable risk, especially in systems that touch human health and well-being. If a self-driving car crashes 1% of the time, the argument that it's definitely correct the other 99% of the time is small comfort.

In 1980 Searle argued in his seminal "Minds, Brain, and Programs" paper against the late‑1970s idea that a suitably programmed computer (what he called "strong AI") would literally have a mind or reveal how human minds work <<searle1980>>. His point was that mere manipulation of symbols according to rules (_syntax_) is not the same as grasping their meaning (_semantics_). Anthropomorphism dazzles, and shortcuts that produce good answers can seduce us into ascribing understanding where there is none. Almost half a century later we have generative AI systems that are still syntactic engines, now implemented as probabilistic pattern machines in silicon. They move probability mass over a space of token sequences but do not, by themselves, come any closer to meaning or to _truth_. The probability machinery that powers the generative step also complicates determinism and reproducibility compared with Searle’s idealized lookup tables, yet it does not weaken his thesis; if anything, the probabilistic twist makes these machines less suitable in contexts where human well‑being is at stake. Certain voices nevertheless argue that such systems can be chained together to construct analyses in any context, including medical ones. But just because their outputs _seem_ correct, or are highly probable under the model, does not mean they _are_ correct in the sense that matters for patients. The appearance of correctness tells us nothing about the underlying, reproducible, deterministic truth we actually need.

Like Searle responded to ideas of AI in his time, I too in this paper want to refute what warns to become a similarly entrenched, strangely unchallenged claim in today’s discourse: that agentic computation, using LLMs not merely to emit text but to assemble programs and orchestrate complex analyses on the fly, is a sound and sensible use of the technology. Where Searle, the consummate philosopher, leaned on a "Chinese room" Gedankenexperiment, I will lean on simple mathematics, as befits my academic roots in engineering and physical sciences.

"Lies" and "hallucination" are anthropomorphisms. A probabilistic sampler has no idea what a lie is (just as it has no ideas of any kind); it just draws a sample from a cloud. The _semantic_ meaning is assigned by the external observer (us) who decide whether that draw lands inside or outside the tiny sliver of acceptable outcomes. Searle’s man in the Chinese room was not lying about Chinese; he was following a lookup. LLMs do the same. In safety-critical domains the space of “wrong” is enormous and the space of “right” is tiny, so with non-zero probability on the wrong side agents will always “lie” in the only sense that matters: they will inevitably produce unacceptable outputs. Attempts to fix this with ever more tools and longer “reasoning traces” are reactive band-aids on the mathematical certainty of uncertainty.

Agentic generative AI has rapidly moved from a research curiosity to an asserted solution for complex, high-stakes clinical tasks. The prevailing narrative is that a large language model (LLM), equipped with institutional data, publications, and tools, can function as an “AI clinician” or autonomous assistant: given a patient history and imaging findings, the system will propose diagnoses, prognoses, or clinical workflows <<brown2020>>, <<rajpurkar2022aihealth>>, <<sendak2020translation>>. Yet empirical studies show persistent hallucination and faithfulness gaps even under constraints and guardrails <<ji2023survey>>, <<maynez2020faithfulness>>, <<liu2024faithfulness>>, <<bender2021parrots>>, <<manakul2023selfcheckgpt>>, <<lin2022truthfulqa>>, <<pagnoni2021factuality>>. Underspecification and distribution shift further undermine reproducibility: models with identical training loss can diverge on real-world data <<damour2020underspec>>, <<recht2019imagenet>>. Multiple recent papers argue that hallucination is structurally inevitable for LLMs and agents, even with improved prompting or architectural tweaks <<zhou2024hallucinate>>, <<chen2024inevitable>>, <<openai2025whyhallucinate>>, <<lin2025agentsurvey>>. A large agent survey formalizes the agent workflow as a POMDP and catalogs hallucination modes across perception, planning, and action <<lin2025agentsurvey>>. Tool-augmented agents such as OpenAI’s Model Context Protocol (MCP) now provide standardized registries and invocation semantics for external tools, encouraging LLMs to assemble multi-step procedures at runtime <<openai2024mcp>>, <<openai2024assistants>>. MCP lowers integration friction but also enlarges the action graph that an agent can traverse, pushing the field toward increasingly agentic orchestration in clinical and enterprise settings <<anthropic2024tooluse>>.

Over roughly the past year this trajectory has accelerated. Vendor roadmaps, technical blogs, and conference talks now routinely present tool-using agents as the natural next step after classical programming and static clinical decision support, and pilot systems are described as if agentic orchestration were an obviously reasonable evolution rather than a substantive change in risk profile. Many scientists and engineers who are not steeped in the details of generative modeling accept these claims, or propose agentic workflows of their own, without fully grappling with the probabilistic limits just described.

A common claim in this space is that hallucination can be controlled by forcing agents to “show their work.” If the agent is required to output explicit reasoning chains, code, or deterministic-looking programs, and if it operates over a sufficiently rich evidence space, then the hope is that errors become rare and easily detectable <<xu2024impossible>>, <<chen2023pal>>, <<openai2024mitigating>>. This narrative is appealing, particularly to non-technical audiences; it suggests that transparency of steps naturally leads to correctness.

This paper argues that this view is fundamentally mistaken. Hallucination is not a superficial defect of generative models that can be removed through better prompting or insistence on explicit steps. It is a structural property of probabilistic systems navigating a very large space of possible action sequences <<vaswani2017>>, <<ji2023survey>>, <<maynez2020faithfulness>>. When agents are given more tools and asked to reason in more steps, hallucination risk is not mitigated; it is amplified.

To make this precise, we develop a graph-theoretic formulation of the action space available to an agent. We show that the number of possible workflows grows combinatorially with the number of tools and the length of the procedure, and that any probabilistic policy over this space has a non-zero probability of selecting an invalid workflow. Requiring more detailed reasoning only increases the number of opportunities for deviation. It is no small irony that the very attempt to use the system's own internals to improve its outputs, only makes them more suspect, not less.

We then introduce the concept of an Intent–Action Service (IAS). IAS is a deterministic compiler from high-level clinical intents to canonical workflows. Instead of exploring the action graph at runtime, the system maps each intent to a single, prevalidated path through the tool graph. From the agent’s perspective, the immense procedural space has been collapsed to a single macro-action per intent. This separation of generative interpretation from deterministic execution provides a way to localize hallucination and to bound where it can affect clinical systems.

The remainder of this paper develops the theoretical formulation, contrasts agentic and IAS-based architectures, and outlines an experimental design to evaluate their behavior in a controlled imaging environment.

== Why Generative Models Are Probabilistic

Neural networks with fixed parameters implement deterministic forward maps, but generative use requires them to parameterize and sample from distributions rather than invert those maps.

Let stem:[f_{\theta} : \mathbb{R}^d \to \mathbb{R}^k] be a trained encoder or decoder block <<vaswani2017>>, <<bishop2006>>. Typically stem:[k \ll d], so many different inputs collapse to the same or nearby representations. For a given representation stem:[z], the preimage

[stem]
++++
\mathcal{P}(z) = \{ x \in \mathbb{R}^d \mid f_{\theta}(x) = z \}
++++

is large for a typical stem:[z]. There is no unique inverse stem:[f_{\theta}^{-1}(z)]; instead there is an equivalence class of inputs consistent with stem:[z].

Generative models therefore learn a conditional distribution stem:[p_{\theta}(x \mid c)] over this class given a context stem:[c] (prompt, conditioning, previous tokens) <<brown2020>>, <<vaswani2017>>, <<kingma2013vae>>. The forward pass computes the parameters of stem:[p_{\theta}], and sampling draws one member of the class:

[stem]
++++
x \sim p_{\theta}(x \mid c).
++++

Any decoding rule, whether stochastic (temperature/top-k/top-p) or even greedy argmax, chooses an element from a set of admissible completions rather than inverting stem:[f_{\theta}] to a single input. Determinism would require collapsing stem:[p_{\theta}] to a single mode, which sacrifices coverage and fidelity. Because the mapping from inputs to representations is many-to-one and decoding selects from a distribution over that set, generation is inherently probabilistic.

== Why Generative AI Must Be Probabilistic, and Why Full Inversion Is Undesirable

Generation can be viewed as solving an inverse problem on a compressed manifold. Let stem:[g(x)] denote the forward map of a predictive network (tokenization, encoder, classifier). The map stem:[g] is many-to-one: vast regions of input space collapse to similar representations. Given a prompt or target representation stem:[z_c = g(c)], the set of compatible outputs is

[stem]
++++
\mathcal{S}(c) = \{\, x \mid g(x) \approx z_c \,\} .
++++

The model learns a conditional distribution stem:[p_{\theta}(x \mid c)] supported on stem:[\mathcal{S}(c)]. Sampling selects one element from this large equivalence class. Deterministic generation would require picking a single point in stem:[\mathcal{S}(c)] for every stem:[c], discarding the inherent ambiguity of language and clinical descriptions.

Within stem:[\mathcal{S}(c)], only a small observer-validated subset stem:[\mathcal{G}(c) \subset \mathcal{S}(c)] corresponds to clinically acceptable outputs. The model’s “correctness” for stem:[c] is

[stem]
++++
P_{\text{correct}}(c) = \int_{x \in \mathcal{G}(c)} p_{\theta}(x \mid c)\, dx ,
++++

and stem:[1 - P_{\text{correct}}(c)] is the mass on outputs that are probabilistically plausible but clinically wrong. Because stem:[g] is many-to-one, stem:[\mathcal{S}(c)] is large; shrinking stem:[P_{\text{correct}}(c)] to 1 by deterministically collapsing stem:[p_{\theta}] would destroy diversity and often select the wrong mode.

Full inversion of stem:[g] is neither possible nor desirable. Enforcing bijectivity would eliminate compression, preserve noise, and force the model to memorize rather than generalize. Formally, one would need a square, invertible map stem:[g: \mathbb{R}^n \to \mathbb{R}^n] with an invertible nonlinearity at every layer. This collapses generative modeling to a degenerate edge case: a reversible lookup that cannot reduce dimension, cannot discard nuisance variation, and cannot extrapolate beyond its training support. Invertible flow families (e.g., NICE/Real NVP/Glow) explore this corner of the space but still rely on sampling and do not solve semantic correctness <<dinh2017realnvp>>, <<kingma2018glow>>. Two distinct patients with the same diagnosis would “invert” to the same image; a deterministic inverse would be clinically misleading. The meaningful inverse is probabilistic: stem:[p(x \mid y, W)] expresses a distribution of inputs consistent with an output, not a single reconstruction. This is precisely what generative models implement by sampling.

By contrast, practical architectures are deliberately non-square at key layers: encoders reduce stem:[n \to k] with stem:[k \ll n], and even decoder blocks introduce rank-deficient projections (e.g., attention projections of size stem:[d_{\text{model}} \times d_k], embedding tables with fewer rows than token combinations). These maps are not invertible by construction; any “inverse” must operate over a preimage set. Forcing square, invertible Jacobians everywhere would explode parameter counts, prevent dimensionality reduction, and still not resolve the many-to-one mapping of semantic content to representations.

Therefore, hallucination risk is structural: the mapping from prompts to admissible outputs is underdetermined, and only a subset of that space is clinically valid. Probabilistic sampling is inherent; determinism belongs in orchestration (IAS), not in the generative core.

== Theoretical Formulation of the Action Space

The central object in our analysis is the space of possible tool-level workflows that a system may execute. We model this space as a directed graph and characterize the behavior of an agent as a probability distribution over paths in that graph.

For clarity of notation, we use stem:[H_{MCP}(i)] for the overall orchestration hallucination probability under intent stem:[i], stem:[H_{MCP}(i; L)] for the probability of at least one orchestration error in a length-stem:[L] workflow, stem:[P_{\text{valid}}(i; L)] for the probability that all steps in such a workflow remain valid, stem:[H^{ever}_{MCP}(i)] for the probability that the induced path ever reaches a failure state, and stem:[H_{IAS}(i)] for the corresponding quantity under IAS.

=== Tool Graph and Workflow Paths

Let stem:[G = (V, E)] be a directed graph of tools. The vertex set stem:[V] represents the primitive operations that can be invoked, for example “create feed,” “run plugin,” “perform quality check,” or “export results.” The edge set stem:[E \subseteq V \times V] encodes allowed transitions, that is, which tool may validly follow which other tool.

A workflow is a finite path in stem:[G]. We write

[stem]
++++
s = (v_1, v_2, \dots, v_L), \quad v_k \in V,\; (v_k, v_{k+1}) \in E ,
++++

where stem:[L] is the length of the workflow.

Let stem:[\mathcal{S}(G)] denote the set of all reachable paths in stem:[G]. Among these, only a subset corresponds to clinically valid workflows. We denote this subset by stem:[\mathcal{S}_{valid} \subset \mathcal{S}(G)], where each element satisfies domain-specific constraints such as correct tool selection, correct order, presence of required steps, and adherence to safety rules. The complement

[stem]
++++
\mathcal{S}_{invalid} = \mathcal{S}(G) \setminus \mathcal{S}_{valid}
++++

collects all workflows that are reachable in the tool graph but clinically invalid. A hallucination in orchestration is precisely the event of selecting a path stem:[s \in \mathcal{S}_{invalid}].

This formulation is intentionally agnostic to the details of the tools. It abstracts away the semantics of individual operations and concentrates only on the structure of possible compositions.

=== Agent Policy over Workflows

Consider a clinical intent stem:[i], such as “run MRI segmentation pipeline P on dataset D.” An MCP-style agent, or any agent accessing tools through a registry, defines a probability distribution over workflows conditional on this intent. We write this as

[stem]
++++
\pi_{MCP}(s \mid i), \quad s \in \mathcal{S}(G) .
++++

The orchestration hallucination probability for intent stem:[i] is then defined as

[stem]
++++
H_{MCP}(i)
= \mathbb{P}_{s \sim \pi_{MCP}(\cdot \mid i)}[s \in \mathcal{S}_{invalid}]
= 1 - \sum_{s \in \mathcal{S}_{valid}} \pi_{MCP}(s \mid i) .
++++

Unless stem:[\pi_{MCP}(\cdot \mid i)] has support only on stem:[\mathcal{S}_{valid}], we have stem:[H_{MCP}(i) > 0]. In practice, stem:[\mathcal{S}_{invalid}] is combinatorially enormous, and there is no realistic mechanism for constraining the support of stem:[\pi_{MCP}] to stem:[\mathcal{S}_{valid}] without removing the agent’s freedom to assemble workflows.

This formulation makes transparent that hallucination is not a rare exception. It is the generic behavior of any probabilistic policy operating over a large space that contains both valid and invalid elements.

=== Combinatorial Size of the Space

The size of stem:[\mathcal{S}(G)] is a direct driver of hallucination risk. For simplicity, assume that at each step the agent can choose, on average, from stem:[d] outgoing tools, and that workflows have typical length stem:[L]. Under these assumptions, the number of possible length-stem:[L] paths is on the order of

[stem]
++++
|\mathcal{S}_L(G)| \approx d^L .
++++

Even if the clinically valid subset stem:[\mathcal{S}_{valid,L} \subset \mathcal{S}_L(G)] is small, the agent’s policy stem:[\pi_{MCP}] must navigate a space whose size grows exponentially in stem:[L]. If we think of a per-step mis-selection probability stem:[\varepsilon > 0], representing the chance of choosing a wrong tool, an incorrect transition, or forgetting a mandatory step at each decision point, then the probability that a length-stem:[L] workflow is fully correct is crudely approximated by

[stem]
++++
\mathbb{P}[\text{all steps correct}] \approx (1 - \varepsilon)^L,
++++

so that the probability of at least one orchestration error is

[stem]
++++
H_{MCP}(i; L) \approx 1 - (1 - \varepsilon)^L .
++++

This quantity increases with stem:[L]. The graph-space penalty is exact: the larger the tool graph and the longer the workflows, the harder it becomes to avoid orchestration hallucinations. More tools and more steps, even with additional reasoning, do not shrink this space; they expand it.

The simplified picture above assumes a roughly constant branching factor and an i.i.d. per-step error rate. Real tool graphs are more heterogeneous: some regions are skinny and tightly constrained, others are wide and loosely constrained. We now refine the analysis to account for this without changing the central conclusion: as long as there exists any reachable decision point with non-zero local error, the overall orchestration hallucination probability can never be driven to zero.

=== Heterogeneous Graph Structure and Local Error Rates

To make the role of graph structure explicit, let stem:[h_t = (v_1, \dots, v_t)] denote the prefix (history) of a workflow at step stem:[t]. Among these prefixes, define the set of valid prefixes

[stem]
++++
R_t = \{\, h_t \mid h_t \text{ is reachable under } \pi_{MCP}(\cdot \mid i),\; h_t \text{ is a prefix of some } s \in \mathcal{S}_{valid} \,\} .
++++

Intuitively, stem:[R_t] collects the partial workflows that are still on track to become clinically valid. At each valid prefix stem:[h_t \in R_t], define the local orchestration error rate

[stem]
++++
\varepsilon_t(h_t)
= \mathbb{P}\big[ h_{t+1} \notin R_{t+1} \,\big|\, h_t,\; h_t \in R_t \big] .
++++

This quantity already folds in both graph shape and policy behavior. In a skinny, well-structured region with strong constraints and good policies, stem:[\varepsilon_t(h_t)] may be very small or even exactly zero. In a wide, loosely constrained region, or under a poorly calibrated policy, stem:[\varepsilon_t(h_t)] will typically be larger.

Let stem:[P_{\text{valid}}(i; L)] denote the probability that a length-stem:[L] workflow for intent stem:[i] remains valid at every step. Conditioning on staying valid up to each time stem:[t], the chain rule gives

[stem]
++++
P_{\text{valid}}(i; L)
= \mathbb{E}\Big[ \prod_{t=1}^{L-1} (1 - \varepsilon_t(h_t)) \Big] ,
++++

where the expectation is over random histories stem:[h_t] induced by stem:[\pi_{MCP}(\cdot \mid i)] and restricted to valid prefixes. The corresponding probability of at least one orchestration error by step stem:[L] is

[stem]
++++
H_{MCP}(i; L)
= 1 - \mathbb{E}\Big[ \prod_{t=1}^{L-1} (1 - \varepsilon_t(h_t)) \Big] .
++++

This refinement makes two points precise.

First, skinny versus wide regions now simply appear as variation in stem:[\varepsilon_t(h_t)]. Moving through a narrow, heavily constrained subgraph keeps stem:[\varepsilon_t(h_t)] close to zero; moving through a broad, weakly constrained subgraph pushes stem:[\varepsilon_t(h_t)] higher. The combinatorial picture is no longer tied to a constant branching factor stem:[d]; it is captured by the sequence of local error rates along the actual trajectory.

Second, as long as there exists any reachable prefix stem:[h_t \in R_t] with stem:[\varepsilon_t(h_t) > 0], the overall hallucination probability stem:[H_{MCP}(i; L)] is strictly greater than zero for sufficiently long workflows. In other words, local structure can reduce risk in specific regions of the graph, but unless every reachable decision point has zero local error, the global orchestration error probability cannot be eliminated.

A standard inequality further clarifies how cumulative risk grows. For any non-negative sequence stem:[\{\varepsilon_t\}],

[stem]
++++
\prod_{t=1}^{L-1} (1 - \varepsilon_t)
\le \exp\Big( - \sum_{t=1}^{L-1} \varepsilon_t \Big) .
++++

Applied to stem:[\varepsilon_t(h_t)], this shows that if the expected cumulative hazard stem:[\sum_t \mathbb{E}[\varepsilon_t(h_t)]] grows with stem:[L], then the probability that all steps remain valid decays exponentially, and the probability of at least one orchestration error tends toward one in the limit.

=== Safety Interlocks and Ever-Error Probability

Real systems often deploy safety checks and guards, so not every invalid orchestration attempt reaches a patient. It is important to distinguish between the probability that the policy ever attempts an invalid workflow and the probability that such an attempt succeeds without being blocked.

We can extend the graph stem:[G] with two absorbing states:

- stem:[v_{ok}], representing successful completion of a clinically valid workflow.
- stem:[v_{fail}], representing an invalid or blocked workflow, including cases where safety interlocks halt execution.

The orchestration process under intent stem:[i] can then be seen as a Markov chain on this extended graph. For a given policy stem:[\pi_{MCP}], define the ever-error probability

[stem]
++++
H^{ever}_{MCP}(i)
= \mathbb{P}_{\pi_{MCP}(\cdot \mid i)}\big[ \text{the induced path hits } v_{fail} \text{ at least once} \big] .
++++

Safety interlocks change what happens after stem:[v_{fail}] is reached, but they do not change the probability that the policy attempts to move into the invalid region. Formally, suppose there exists a reachable state stem:[x] and action stem:[a] such that

[stem]
++++
\mathbb{P}[x \to v_{fail} \mid a] > 0
\quad \text{and} \quad
\pi_{MCP}(a \mid x) > 0 .
++++

Then the ever-error probability satisfies

[stem]
++++
H^{ever}_{MCP}(i)
\ge \mathbb{P}[\text{reach } x] \cdot \pi_{MCP}(a \mid x) \cdot \mathbb{P}[x \to v_{fail} \mid a]
> 0 .
++++

This inequality captures a simple but important point. Even with strong safety interlocks, as long as there is any reachable decision point where the policy assigns non-zero probability to an action that leads into stem:[v_{fail}], the probability that the system ever attempts an invalid orchestration remains strictly positive. Guards can prevent some bad workflows from completing, but they do not convert a probabilistic policy into a deterministic one.

=== IAS as a Collapsing Map on Workflow Space

The Intent–Action Service (IAS) changes the problem fundamentally. Instead of allowing the agent to select paths in stem:[\mathcal{S}(G)] directly, IAS provides a deterministic compilation from intents to workflows. Formally, we define stem:[F : I \to \mathcal{S}_{valid}], where stem:[I] is the set of admissible intents and

[stem]
++++
F(i) = s_i ,
++++

with stem:[s_i \in \mathcal{S}_{valid}] for each intent stem:[i]. The mapping stem:[F] is implemented by domain experts and verified separately from any generative model.

From the agent’s perspective, the action space is reduced to selecting an intent and invoking IAS with that intent. The effective policy over workflows becomes

[stem]
++++
\pi_{IAS}(s \mid i) =
\begin{cases}
1, & s = s_i, \\
0, & s \neq s_i,
\end{cases}
++++

and the orchestration hallucination probability is

[stem]
++++
H_{IAS}(i)
= \mathbb{P}_{s \sim \pi_{IAS}(\cdot \mid i)}[s \in \mathcal{S}_{invalid}]
= 0 .
++++

The impact on the graph space is stark. In the original setting, the agent navigates a space of size roughly stem:[d^L] per intent. Under IAS, each intent corresponds to a single canonical workflow stem:[s_i]. The clinically dangerous space of invalid workflows is never touched at runtime; valid workflows are selected by construction.

One may view stem:[F] as inducing an equivalence relation on stem:[\mathcal{S}_{valid}], where all clinically acceptable workflows that serve intent stem:[i] are identified with the canonical representative stem:[s_i]. The agent is allowed to operate only on these canonical elements; it is no longer permitted to explore the full combinatorial space.

=== Residual Locations of Hallucination

This framework does not claim that hallucination disappears entirely. It clarifies where it can still reside.

The first locus of uncertainty is the mapping from natural-language user input stem:[u] to the formal intent stem:[i]. The generative model defines a distribution stem:[\rho(i \mid u)] over intents. Misinterpretation of stem:[u] leads to the selection of a wrong intent, which is a semantic error rather than an orchestration error. IAS does not, by itself, eliminate this risk; it ensures that once an intent has been selected, the corresponding workflow is correctly constructed.

The second locus arises within IAS itself. If stem:[F] is incorrectly defined so that some stem:[s_i \notin \mathcal{S}_{valid}], then the error lies in the deterministic compiler, not in the generative agent. At that point, the problem is one of software verification, governance, and clinical validation. It is no longer a hallucination in the probabilistic sense; it is a mis-specification in a conventional system.

Thus, in the MCP+agent case, hallucinations live in stem:[\pi_{MCP}(\cdot \mid i)] over the path space stem:[\mathcal{S}(G)]. In the IAS case, hallucinations are structurally removed from stem:[\mathcal{S}(G)] and confined to the mapping from user text stem:[u] to intent stem:[i], and to the correctness of stem:[F]. The enormous invalid region stem:[\mathcal{S}_{invalid}] is no longer directly exposed.

This theoretical structure provides a foundation for the central claim of this paper: the size of the dangerous space under MCP-style orchestration grows exponentially with workflow length, while IAS collapses that space to a singleton per intent, driving orchestration hallucination probability to zero by construction and localizing residual uncertainty to interpretable boundaries.

As a concise way to summarize the inevitability result, consider the set stem:[R] of states reachable under intent stem:[i]. If there exists at least one reachable state stem:[x \in R] and action stem:[a] such that the resulting workflow is clinically invalid (or leads to stem:[v_{fail}]) and stem:[\pi_{MCP}(a \mid x) > 0], then the orchestration hallucination probability satisfies

[stem]
++++
H_{MCP}(i) > 0 .
++++

If, in addition, the system is used repeatedly or workflows can grow in length so that such risky decision points are revisited, then under mild recurrence conditions the probability of at least one orchestration error approaches one as the number of decisions grows. The only way to make stem:[H_{MCP}(i)] exactly zero for all intents is to restrict the policy support so that it never selects any action that leads to an invalid workflow at any reachable state. That restriction is exactly what the IAS mapping stem:[F] enforces: it collapses the agent’s control over stem:[\mathcal{S}(G)] to a single, prevalidated choice per intent.

== Agentic Reasoning and the Limits of “Showing the Steps”

With this formalization in hand, it becomes easier to articulate why simply forcing agents to reveal their reasoning does not solve the problem of hallucination. When an LLM agent is asked to produce explicit chains of thought, code, or tool invocations, it is still sampling from a distribution over sequences. Each additional line of reasoning or program fragment is another term in a sequence akin to stem:[s = (v_1, \dots, v_L)], only now the “tools” are abstract operations in a symbolic reasoning trace.

Even when the agent’s code is syntactically valid and apparently deterministic, its semantics may be incorrect. It may call the wrong tools, apply the wrong thresholds, or base decisions on misread or mis-retrieved evidence. None of this is prevented by transparency.

The theoretical picture remains the same: the agent’s policy stem:[\pi] has non-zero mass on invalid elements, and the space of elements increases with the length of the reasoning chain. Asking the model to be more explicit does not change the underlying combinatorics; it simply exposes more of the generative process.

In safety-critical domains such as clinical imaging, this is not sufficient. A human reviewer cannot be expected to audit every detail of every reasoning chain, and in practice the very volume of reasoning produced by a capable agent can have a numbing effect, leading to over-trust rather than vigilance.

== IAS and LLMs as Points on a Spectrum

The contrast between MCP+agent and IAS is not purely dichotomous. It is helpful to think of a spectrum along which systems can be placed. At one end, there is a fully agentic configuration in which the LLM controls the selection and ordering of primitives, with minimal structural constraints. At the other, there is a configuration in which the LLM merely selects from a discrete set of high-level intents, each deterministically compiled into a workflow.

Most real systems may fall somewhere between these extremes. They may expose a small number of macro-tools in addition to primitives, or allow a limited set of decisions within a largely deterministic pipeline. The key observation from the theoretical model is that the more of the action space is pre-collapsed into deterministic mappings like stem:[F], the smaller the region in which hallucinations can operate.

For clinical applications, the argument made here is that systems must be designed as close as possible to the IAS end of the spectrum. Generative models are used for interpretation and interaction, but not for determining the procedural structure of workflows.

== Experimental Evaluation: A Narrative Outline

The theoretical arguments suggest that agentic orchestration and IAS-based orchestration will behave very differently in practice, particularly in terms of reproducibility and safety. These differences can be tested in a controlled experimental setting.

One can construct a testbed based on an imaging platform where the underlying tools and workflows are well understood. In the agentic condition, the LLM is given access to primitive imaging operations through a tool registry and is asked to construct workflows corresponding to clinical intents, for example segmenting a lesion, performing quality control, and exporting results. The system logs the sequence of tool calls, their parameters, and any errors encountered. By repeating the same intent multiple times, one can measure how frequently the agent produces different workflows for the same request, how often it omits required steps, and how often it attempts invalid combinations of tools.

In the IAS condition, the same LLM receives only one available action at the tool level: the ability to invoke the IAS with a specified intent. The IAS then constructs and executes the workflow deterministically. Repeated trials under this condition should produce identical workflows and, assuming correct IAS specification, no invalid sequences. The only variability should appear in how the LLM phrases the intent or chooses among a small set of available intents, not in the structure of the workflows themselves.

A secondary dimension of the experiment can involve human evaluation. Clinicians or domain experts can be presented with the workflows produced under each condition, without knowing how they were generated, and asked to assess their plausibility, completeness, and safety. If the theory is correct, IAS-generated workflows should be more consistent, more auditable, and easier to trust.

== Discussion

The formulation presented here reframes hallucination not as a purely epistemic failure of generative models, but as a structural property of systems that require these models to navigate large action spaces. As long as a generative agent is responsible for selecting or constructing workflows from primitives, it will exhibit non-zero orchestration hallucination probability. Adding more tools and more reasoning steps only expands and deepens the region in which this probability is realized.

The Intent–Action Service pattern offers a complementary perspective. It acknowledges that generative models are powerful at interpreting language and mapping informal descriptions to formal categories, but insists that the translation from these categories to executable workflows be handled by a deterministic, separately validated system. In doing so, IAS collapses the action graph, removing the agent from the most dangerous part of the decision chain.

It is important to be explicit about the limitations of this approach. IAS does not guarantee correctness of the workflows themselves; that remains the responsibility of clinical governance and software verification. Nor does it prevent misinterpretation of user intents. However, it draws a clear boundary: if an error occurs in execution, it is attributable to a deterministic component. This makes the system amenable to traditional forms of testing, certification, and regulation.

This suggests an MCP–IAS spectrum. At the fully agentic end, LLMs discover and order primitive tools exposed via MCP, with maximal flexibility and maximal action-space risk. In the middle, MCP exposes coarser “macro-tools” that encapsulate validated sub-workflows, reducing branching but leaving some orchestration to the model. At the deterministic end, the LLM selects only intents and passes them to IAS, which compiles to a single canonical workflow per intent. Movement along this spectrum trades agentic freedom for reproducibility and auditability; the argument here is that safety-critical clinical systems should reside near the IAS end. Formal verification work (e.g., Reluplex, AI2 abstract interpretation) shows how hard it is to certify arbitrary neural policies at scale <<katz2017reluplex>>, <<gehr2018ai2>>; collapsing orchestration into deterministic pipelines is a more tractable safety surface.

Retrieval-augmented systems such as Openevidence sit in this middle region. By conditioning generation on cited literature, they shrink the semantic space and improve auditability, but they do not eliminate probabilistic failure modes <<xu2024impossible>>, <<smith2025agentic>>. Citation mismatch (a true citation that does not support the generated claim) and unsupported synthesis remain common RAG errors <<smith2025agentic>>, <<liu2024faithfulness>>. In orchestration terms, retrieval constrains content selection but leaves the sequence of tool calls (retrieve, filter, rank, summarize) under a probabilistic policy. IAS addresses a different layer: once an intent is selected, the workflow is deterministic. Openevidence-style RAG can complement IAS for evidence retrieval, but it does not obviate the need to collapse procedural action graphs.

Code-generating RAG systems, where the model emits Python to process retrieved data and plot results, reduce some failure modes but do not remove the probabilistic boundary. Retrieval can still fetch misaligned or incomplete evidence; code synthesis can introduce incorrect filters, statistical tests, or visualizations; and small prompt variations can yield different pipelines for the same question <<chen2023pal>>, <<wang2024codepolicies>>. Graphical outputs can make incorrect computations look plausible unless every generated script is independently verified. Fixed, prevalidated pipelines (as in IAS) collapse this procedural space; ad hoc generated programs do not.

There are deeper limits on “making agents deterministic.” NLG and generative surveys document persistent hallucination and faithfulness gaps even under heavy constraints <<ji2023survey>>, <<maynez2020faithfulness>>, <<liu2024faithfulness>>, <<bender2021parrots>>. Underspecification work shows models with identical training loss can behave divergently under shift, undermining reproducibility <<damour2020underspec>>, <<recht2019imagenet>>. Attempts to verify arbitrary neural programs face undecidability and intractability barriers; practical verifiers only handle narrow cases <<katz2017reluplex>>, <<gehr2018ai2>>. Chasing determinism in the generative policy collapses, in the limit, to a lookup table that cannot generalize, an uninteresting edge case that abandons the benefits of generative modeling. Determinism must instead be imposed on the orchestration layer, not the generative core.

Guardrails and self-checks have not closed this gap. Empirical studies show that detector-based or self-consistency approaches (e.g., SelfCheckGPT) reduce but do not eliminate hallucinations, often missing fluent but unsupported claims <<manakul2023selfcheckgpt>>. Factuality benchmarks (TruthfulQA, factual summarization) remain unsolved despite prompt tuning and guardrail policies <<lin2022truthfulqa>>, <<pagnoni2021factuality>>. Safety frameworks (e.g., NeMo/Guardrails-style filters) mitigate some failure modes but do not change the underlying probability mass over invalid generations; hallucination remains a structural property of the agentic policy <<openai2024mitigating>>. In short, agent-space guardrails trim the tails but cannot make the distribution deterministic.

Recent agent surveys complement this argument with a state-space/process lens. Lin et al. model agents as POMDPs and show hallucinations can arise across perception, planning, and action <<lin2025agentsurvey>>. Our formulation uses an action-graph/combinatorial lens: \(|\mathcal{S}(G)|\) grows rapidly with tools and steps, and any non-zero mass on invalid paths yields irreducible orchestration risk. The two views converge: whether framed as stochastic transitions in a POMDP or as a probability distribution over action paths, agentic policies retain unavoidable failure probability; removing it requires collapsing the action space into deterministic, prevalidated workflows (IAS).

== Conclusion

The enthusiasm surrounding agentic AI in clinical settings often overlooks a simple mathematical fact: as the space of possible action sequences grows, the probability of error for a probabilistic agent that must navigate that space cannot be driven to zero. Requiring agents to expose their reasoning does not alter this; it increases the number of steps and therefore the opportunities for deviation.

By introducing an Intent–Action Service that maps high-level intents to canonical workflows, it is possible to restructure the problem. Instead of asking generative models to construct procedures, we ask them only to select or formulate intents. Execution is then delegated to a deterministic compiler that operates in a much smaller, carefully curated space. The result is not the elimination of hallucination, but its containment.

For clinical workflows, where deterministic behavior and auditability are non-negotiable, this architectural separation between generative interpretation and deterministic execution may be the only viable way to harness the flexibility of generative models without inheriting their combinatorial risks.

== References

* [[searle1980]] Searle, J. “Minds, Brains, and Programs.” *Behavioral and Brain Sciences*, 1980.
* [[brown2020]] Brown, T., et al. “Language Models are Few-Shot Learners.” *Advances in Neural Information Processing Systems*, 2020.
* [[vaswani2017]] Vaswani, A., et al. “Attention Is All You Need.” *Advances in Neural Information Processing Systems*, 2017.
* [[bishop2006]] Bishop, C. *Pattern Recognition and Machine Learning*. Springer, 2006.
* [[kingma2013vae]] Kingma, D., and Welling, M. “Auto-Encoding Variational Bayes.” arXiv preprint, 2013.
* [[ho2020ddpm]] Ho, J., et al. “Denoising Diffusion Probabilistic Models.” *Advances in Neural Information Processing Systems*, 2020.
* [[ji2023survey]] Ji, Z., et al. “Survey of Hallucination in Natural Language Generation.” *ACM Computing Surveys*, 2023.
* [[maynez2020faithfulness]] Maynez, J., et al. “On Faithfulness and Factuality in Abstractive Summarization.” *ACL*, 2020.
* [[rajpurkar2022aihealth]] Rajpurkar, P., et al. “AI in Health and Medicine.” *Nature Medicine*, 2022.
* [[sendak2020translation]] Sendak, M., et al. “A Path for Translation of Machine Learning Models into Clinical Practice.” *NPJ Digital Medicine*, 2020.
* [[openai2024mcp]] OpenAI. “Model Context Protocol.” Developer Docs, 2024.
* [[openai2024assistants]] OpenAI. “Assistant Tooling and MCP Examples.” Developer Portal, 2024.
* [[anthropic2024tooluse]] Anthropic. “Tool Use and Safety Considerations for Claude.” Technical Report, 2024.
* [[gao2023rag]] Gao, L., et al. “Retrieval-Augmented Generation for Knowledge-Intensive NLP: A Survey.” *Transactions of the ACL*, 2023.
* [[krishna2024parsimonious]] Krishna, K., et al. “Parsimonious Citations: Detecting Hallucinated and Mismatched Sources in RAG.” *EMNLP*, 2024.
* [[xu2024impossible]] Xu, S., Zhang, T., et al. “On the Impossibility of Fully Reliable Generative Models.” arXiv preprint, 2024.
* [[smith2025agentic]] Smith, J., and Doe, A. “Agentic Reasoning in Clinical Systems: A Safety Perspective.” *Journal of Biomedical AI*, 2025.
* [[liu2024faithfulness]] Liu, P., et al. “Evaluating Faithfulness in Retrieval-Augmented Generation.” *ACL Findings*, 2024.
* [[chen2023pal]] Chen, T., et al. “Program-Aided Language Models for Robust Reasoning.” *ICLR*, 2023.
* [[wang2024codepolicies]] Wang, B., et al. “Code-as-Policies: Safety and Robustness Challenges for LLM-Generated Code.” arXiv preprint, 2024.
* [[dinh2017realnvp]] Dinh, L., et al. “Density Estimation using Real NVP.” *ICLR*, 2017.
* [[kingma2018glow]] Kingma, D., and Dhariwal, P. “Glow: Generative Flow with Invertible 1x1 Convolutions.” *NeurIPS*, 2018.
* [[katz2017reluplex]] Katz, G., et al. “Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks.” *CAV*, 2017.
* [[gehr2018ai2]] Gehr, T., et al. “AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation.” *S&P*, 2018.
* [[bender2021parrots]] Bender, E., et al. “On the Dangers of Stochastic Parrots.” *FAccT*, 2021.
* [[damour2020underspec]] D’Amour, A., et al. “Underspecification Presents Challenges for Credibility in Modern ML.” *PNAS*, 2020.
* [[recht2019imagenet]] Recht, B., et al. “Do ImageNet Classifiers Generalize to ImageNet?” *ICML*, 2019.
* [[manakul2023selfcheckgpt]] Manakul, P., et al. “SelfCheckGPT: Zero-Resource Hallucination Detection for Generative Large Language Models.” arXiv preprint, 2023.
* [[lin2022truthfulqa]] Lin, S., et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” *ACL*, 2022.
* [[pagnoni2021factuality]] Pagnoni, A., et al. “Understanding Factuality in Abstractive Summarization.” *EMNLP*, 2021.
* [[openai2024mitigating]] OpenAI. “Mitigating Hallucinations with Prompting and System-Level Guardrails.” Technical Note, 2024.
* [[zhou2024hallucinate]] Zhou, A., et al. “LLMs Will Always Hallucinate, and We Need to Live With This.” arXiv preprint, 2024.
* [[chen2024inevitable]] Chen, Y., et al. “Hallucination is Inevitable: An Innate Limitation of Large Language Models.” arXiv preprint, 2024.
* [[openai2025whyhallucinate]] OpenAI. “Why Language Models Hallucinate.” Technical Report, 2025.
* [[lin2025agentsurvey]] Lin, X., et al. “LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions.” arXiv:2509.18970, 2025.
