=== 2.4 External Intent-Action Service (IAS)

The third architectural path introduces an independent Intent-Action Service (IAS) positioned as a dedicated intermediary layer between clients and CUBE. Unlike embedding orchestration logic within the UI (Section 2.1) or absorbing it into CUBE itself (Section 2.3), this approach recognizes intent translation as a distinct architectural concern deserving its own service boundary. The IAS provides a procedural, task-oriented API to clients while preserving CUBE's declarative Collection+JSON substrate intact. This design embodies what Parnas termed *information hiding* [14]: each layer encapsulates concerns appropriate to its responsibility, exposing only what its consumers require.

The figure below illustrates this externalized architecture. Note that visualization intents bypass the IAS entirely, flowing directly to CUBE's Cj API, while behavioral intents route through the IAS for orchestration.

....
┌─────────────────────────┐              ┌─────────────────────────┐
│  user visualization     │              │  user behavioral        │
│  intent ("show feeds")  │              │  intent ("anonymize")   │
└────────────┬────────────┘              └────────────┬────────────┘
             │                                        │
             │                                        │
             │                                        │
   ┌─────────┼────────────────────────────────────────┼──────────┐
   │         │                 ChRIS UI               │          │
   ├─────────┼────────────────────────────────────────┼──────────┤
   │         │            React components            │          │
   │         │         (presentation only)            │          │
   ├─────────┼────────────────────────────────────────┼──────────┤
   │         │    js thin client library              │          │
   └─────────┼────────────────────────────────────────┼──────────┘
             │                                        │
             │                                        │
             │                              ┌─────────┼──────────┐
             │                              │         │   IAS    │
             │                              │  ╔══════▼════════╗ │
             │                              │  ║ Intent Logic  ║ │
             │                              │  ║ Orchestrator  ║ │
             │                              │  ╚══╦═══╦═══╦════╝ │
             │                              │     │   │   │      │
             │                              └─────┼───┼───┼──────┘
             │                                    │   │   │
             │                                    │   │   │
   ┌─────────┼────────────────────────────────────┼───┼───┼──────┐
   │         │                  CUBE              │   │   │      │
   ├─────────┼────────────────────────────────────┼───┼───┼──────┤
   │         ▼         Collection+JSON API        ▼   ▼   ▼      │
   ├─────────────────────────────────────────────────────────────┤
   │       CUBE Python/Django internals                          │
   │       (declarative substrate preserved)                     │
   └─────────────────────────────────────────────────────────────┘
....

This geometry reveals several critical properties:

1. **Dual-path routing preserves CUBE's declarative purity.** Simple queries—"show feeds," "list plugins"—continue to use CUBE's hypermedia API directly. Only complex procedural workflows—"anonymize data," "execute pipeline"—invoke the IAS. This selective routing maintains CUBE's original design philosophy while addressing the orchestration gap.

2. **The IAS operates as a Backend-for-Frontend (BFF).** Clients interact with a simplified, task-oriented interface rather than navigating CUBE's resource graph. This pattern, documented extensively by Newman [23] and Richardson [26], allows the frontend to remain focused on presentation while delegating orchestration to a specialized backend proxy that understands both client intent and backend semantics.

3. **Service boundaries align with evolutionary rates.** Intent logic evolves rapidly as workflows mature and new use cases emerge. CUBE's declarative model evolves slowly as core entities stabilize. By separating these concerns into independent services, the architecture enables what Ford et al. define as *evolutionary architecture* [38]: each component can change at its natural pace without cascading modification across boundaries. This directly addresses the coupling problem identified in Sections 2.1 and 2.3.

4. **Orchestration state resides outside CUBE.** The IAS manages stateful workflows—tracking progress, handling retries, coordinating multi-step transactions—without polluting CUBE's stateless REST semantics. This mirrors the saga pattern introduced by Garcia-Molina and Salem [27], where a coordinator service sequences operations across distributed resources while maintaining transactional consistency through compensating actions.

==== Architectural Advantages

The externalized IAS model offers several structural benefits over the alternatives analyzed in prior sections.

*Separation of concerns.* CUBE remains a pure declarative substrate describing entities and relationships. The IAS provides procedural orchestration. Clients implement presentation and user interaction. Each service boundary encapsulates a single responsibility, reducing coupling and increasing cohesion. This separation directly applies Evans's bounded context principle [28]: each service operates within a well-defined semantic boundary, minimizing the conceptual surface area that developers must understand.

*Loose coupling and independent deployability.* Because the IAS communicates with CUBE exclusively through its REST API, the two services remain loosely coupled. CUBE can be upgraded, scaled, or even reimplemented without affecting IAS logic, provided the API contract is preserved. Conversely, intent workflows can be added, modified, or deprecated without CUBE involvement. This operational independence, central to microservices philosophy [24], reduces deployment risk and accelerates iteration velocity.

*Client ecosystem enablement.* With the IAS providing a stable procedural interface, diverse clients—web UIs, CLI tools, mobile apps, automation scripts—can share orchestration logic without duplication. A Python client and a JavaScript client invoke the same IAS endpoint and receive the same behavior. This eliminates the fragmentation described in Section 2.1, where each client reimplemented orchestration independently. The resulting ecosystem effect mirrors Conway's Law in reverse [29]: by structuring the architecture to enable diverse teams, the system naturally supports diverse implementations.

*Observability and operational control.* Centralizing orchestration in the IAS creates a natural instrumentation point. Intent requests, workflow progress, error rates, and latency distributions can be monitored at the service boundary. This visibility, difficult to achieve when orchestration is scattered across clients or embedded within CUBE, enables operational insights and performance optimization. The IAS becomes both an orchestration engine and a telemetry hub.

==== Additional Consideration: Potential for LLM Integration

An additional potential benefit of the IAS architecture is compatibility with LLM-driven tooluse patterns. Task-oriented intent endpoints with clear semantic meaning may prove easier for language models to invoke than navigating hypermedia graph structures [30][31][32]. Where direct CUBE usage requires multi-step orchestration across Collection+JSON links, IAS intents encapsulate workflows as single operations. Whether this theoretical advantage translates to practical benefit depends on LLM capability evolution, API design quality, and user acceptance—all of which require empirical validation beyond this paper's scope. We note this possibility as a secondary benefit but emphasize that the IAS proposal rests primarily on addressing current client ecosystem challenges rather than speculative AI integration scenarios.

==== Implementation Considerations

Deploying the IAS as an independent service introduces practical considerations absent from the embedded alternatives. The service must maintain its own API contract, authentication and authorization mechanisms, error-handling semantics, and operational instrumentation. These responsibilities, while increasing system complexity, also confer modularity and evolutionary flexibility.

From a security standpoint, the IAS architecture introduces authentication and credential management challenges. The service must authenticate clients, authorize intent-level operations, and interact with CUBE on behalf of users without creating credential distribution vulnerabilities. One possible solution is credential brokering rather than credential storage: systems like `authCore` [34] provide encrypted per-user credential vaults where services request operations without handling raw credentials. This defense-in-depth approach ensures that compromising the IAS exposes no credential material while maintaining comprehensive audit trails. However, multiple authentication strategies remain viable pending implementation requirements and security review.

The IAS API design should prioritize clarity and usability. Where CUBE's Cj API exposes resources and link relations, the IAS exposes *tasks* and *workflows*. Endpoints like `POST /intents/anonymize`, `GET /workflows/{id}/status`, and `POST /intents/execute-pipeline` convey procedural semantics directly. OpenAPI specifications [33] or GraphQL schemas provide machine-readable contracts, enabling client code generation and reducing integration friction. This procedural clarity stands in deliberate contrast to CUBE's declarative model, offering clients the interface style most appropriate to their needs.

Operational deployment follows standard microservices practice. The IAS runs as a containerized service, horizontally scalable and independently versioned. It communicates with CUBE over HTTP, introducing network latency but gaining deployment independence. Kubernetes, Docker Swarm, or similar orchestration platforms manage service lifecycle, health checks, and traffic routing. Observability tooling—Prometheus, Grafana, OpenTelemetry—instruments request flows, enabling performance monitoring and debugging across service boundaries.

==== The SeaGaP Pattern: Foundational Intent Model

The practical success of the IAS architecture depends on identifying representative workflow patterns that intent abstractions can effectively encode. Analysis of ChRIS deployments reveals a dominant pattern we term *SeaGaP* (SEArch/GAther/Pipeline), which characterizes the vast majority of clinical imaging workflows:

1. *Search:* Locate imaging data within CUBE's filesystem using clinical identifiers (patient accession numbers, study dates, protocol names).

2. *Gather:* Filter search results by imaging parameters (modality, sequence type, acquisition protocol) and aggregate selected data into a ChRIS feed—the atomic unit of processable data in the CUBE model.

3. *Pipeline:* Execute a ChRIS pipeline (or package operation for data export) on the gathered feed, passing pipeline-specific parameters as needed.

This pattern currently forces clients to orchestrate 8-15 discrete REST operations: CUBE filesystem queries, Collection+JSON parsing, conditional filtering logic, feed creation with proper structural metadata, data-to-feed linking, pipeline instantiation with parameter validation, and asynchronous polling for completion status. Each operation multiplies failure modes, error-handling complexity, and client-side implementation divergence—precisely the fragmentation problems documented in Section 2.1.

The IAS eliminates this orchestration burden by collapsing multi-step workflows into single declarative requests. Consider two core examples representing the foundational intent types:

.Example 1: Download intent
*Use case:* "Download the MPRAGE scan of PatientID 12344"

Current approach requires 8-15 REST calls for filesystem search, filtering, feed creation, data linking, zip plugin instantiation, and polling.

IAS intent-based approach:
[source,http]
----
POST /intents/download HTTP/1.1
Content-Type: application/json

{
  "search": {"PatientID": "12344"},
  "gather": {"ProtocolName": "MPRAGE"}
}

→ 202 Accepted
{
  "jobId": "intent-789",
  "statusUrl": "/intent/jobs/intent-789"
}
----

.Example 2: Pipeline intent
*Use case:* "Run FreeSurfer pipeline on T1-weighted scan of PatientID 12344"

IAS intent-based approach:
[source,http]
----
POST /intents/pipeline HTTP/1.1
Content-Type: application/json

{
  "search": {"PatientID": "12344"},
  "gather": {"ProtocolName": "T1"},
  "pipeline": {
    "name": "freesurfer-pipeline",
    "parameters": {
      "reconstruct_all": true,
      "subjects_dir": "/output",
      "parallel": 4
    }
  }
}

→ 202 Accepted
{
  "jobId": "intent-790",
  "statusUrl": "/intent/jobs/intent-790"
}
----

The intent abstraction encapsulates the entire workflow, moving orchestration complexity from distributed client code into a centralized, testable, maintainable service. This transformation directly addresses the client ecosystem stagnation problem: developers build applications using high-level task APIs rather than navigating low-level resource graphs. These two core intents—`download` and `pipeline`—cover the overwhelming majority of ChRIS usage patterns.

==== API Design Recommendation: Phased Dual-Interface

The SeaGaP pattern establishes *what* intents should accomplish. The question of *how* to structure intent endpoints admits two complementary approaches, each serving different user needs. We recommend implementing both via a phased strategy.

*Action-first (task-oriented) design* places the operation at the URL's root, mirroring natural language task framing:

[source,http]
----
POST /intents/download
POST /intents/pipeline
----

This structure aligns with workflow-oriented thinking: "I want to *download* some data" or "I want to run a *pipeline*" (verb → object). The approach provides concise URLs and intuitive semantics for task-driven interfaces—ideal for interactive UI workflows and CLI commands. The trade-off is placing verbs in URLs, a deviation from strict REST resource modeling, and limited programmatic discoverability.

*Data-first (resource-oriented) design* inverts this structure, beginning with data location and enabling action discovery:

[source,http]
----
# Discover available operations
OPTIONS /intent/data/CUBE/PatientID/12344
→ 200 OK
X-Available-Actions: download, pipeline

# Execute discovered operation
POST /intent/data/CUBE/PatientID/12344/actions/pipeline
Content-Type: application/json

{
  "protocolFilter": "T1",
  "pipeline": {
    "name": "freesurfer-pipeline",
    "parameters": {"reconstruct_all": true}
  }
}
----

This structure conforms to RESTful resource modeling, supports programmatic discovery via HTTP OPTIONS, and makes data location explicit within CUBE's filesystem. It excels for batch processing scripts and data engineering workflows where operations target specific data collections. The cost is increased URL verbosity and an additional HTTP round-trip for discovery.

Neither approach dominates universally. The table below compares key characteristics:

[cols="3,4,4",options="header"]
|===
|Criterion |Action-First |Data-First

|REST compliance
|Verbs in URLs (anti-pattern per Fielding [1])
|Nouns as resources (RESTful)

|Discoverability
|Requires external documentation
|`OPTIONS` returns available actions

|Primary use case
|Interactive workflows, task-driven UIs
|Batch processing, data exploration

|URL complexity
|Concise (e.g., `/intents/download`)
|Hierarchical (e.g., `/intent/data/CUBE/.../actions/pipeline`)

|Data source specification
|Query parameter or request body
|Explicit in path (`/CUBE/PatientID/...`)

|Learning curve
|Immediate (matches natural language)
|Requires understanding resource hierarchy
|===

*Implementation recommendation:* Deploy both interfaces routing to a unified internal orchestration engine. The routing layer translates URL patterns into a shared intent representation:

[source,python]
----
class Intent:
    search: SearchCriteria    # Patient ID, study date, etc.
    gather: GatherCriteria    # Protocol filters, modality selection
    pipeline: PipelineAction  # Pipeline name, pipeline-specific parameters
----

*Phased rollout strategy:*

* *Phase 1 (Weeks 1-3):* Implement action-first API only. Focus on 2 core SeaGaP intents (`download` and `pipeline`) to validate core architecture and user acceptance. This minimizes initial complexity while delivering immediate value.

* *Phase 2 (Weeks 11+):* Add data-first API for power users. Implement alongside action-first without disrupting existing clients. Target audience: batch processing scripts, automated data pipelines, programmatic exploration tools.

This dual-interface strategy maximizes usability across user types—interactive users benefit from concise action-first URLs, while automation workflows leverage data-first discoverability—without fragmenting the underlying implementation. Both interfaces share orchestration logic, state management, error handling, and observability instrumentation.

==== Error Handling Strategy

Robust error handling transforms theoretical architectural benefits into practical operational reliability. The SeaGaP pattern's three-phase structure demands phase-specific error classification, actionable recovery guidance, and partial progress preservation.

*Search phase failures* distinguish between transient conditions (CUBE filesystem temporarily unavailable, include `retryable: true` flag) and permanent conditions (patient identifier does not exist in CUBE). Error responses should indicate search status, suggest verification steps, and provide retry timing guidance for transient failures.

*Gather phase failures* handle ambiguity and absence systematically. When multiple scans match gather criteria without sufficient disambiguation, return a `409 Conflict` response listing all matches with relevant metadata (scan dates, series identifiers) and requiring explicit `gatherStrategy` selection (`latest`, `earliest`, `all`, or date-specific filter). When no scans match the protocol filter, return `404 Not Found` with available alternatives from CUBE—enabling self-service correction rather than support escalation.

*Pipeline phase failures* preserve expensive intermediate state. Feed creation and data linking operations completed during the Gather phase remain intact even if pipeline execution fails. Error responses include feed identifiers, pipeline execution logs, and explicit retry endpoints (`/intents/retry/{jobId}`) that resume from the failure point without re-executing successful phases. For the `download` intent specifically, packaging failures (e.g., zip plugin errors) similarly preserve the created feed for retry or manual inspection.

*Asynchronous execution patterns* prevent timeout cascades. All intent requests return `202 Accepted` immediately with job tracking identifiers. Status polling endpoints (`/intent/jobs/{jobId}`) provide current phase, completion percentage, and estimated remaining time. This pattern supports workflows ranging from sub-second anonymization to multi-hour FreeSurfer reconstruction without HTTP timeout complications.

*Error response structure:*
[source,json]
----
{
  "error": "ErrorTypeName",              // Machine-readable error class
  "message": "Human-readable description",
  "phase": "search|gather|pipeline",      // Attribution to SeaGaP phase
  "retryable": true,                      // Transient vs. permanent
  "suggestion": "Actionable next step",   // Recovery guidance
  "context": {...}                        // Phase-specific details
}
----

This structured approach enables programmatic error handling in client code while maintaining human readability for debugging. Error types map directly to HTTP status codes following REST conventions: `404` for missing resources, `409` for conflicts requiring disambiguation, `503` for service unavailability, `500` for unexpected failures requiring investigation.

==== Trade-offs and Limitations

While the externalized IAS model addresses many architectural shortcomings, it introduces trade-offs. The additional service layer increases operational complexity: another component to deploy, monitor, version, and debug. Network communication between IAS and CUBE adds latency compared to in-process orchestration (as in Section 2.3). Distributed system failure modes—network partitions, timeout cascades, inconsistent state—become operational concerns requiring careful design and testing.

Moreover, the IAS must evolve its own API contract over time. Unlike the embedded approach where orchestration logic remains internal to CUBE, the IAS exposes a public interface that clients depend upon. Breaking changes require coordination and migration planning, much like CUBE's own API. This shifts complexity from implementation (spread across clients) to interface design (centralized in the IAS), a trade that favors ecosystem consistency but demands disciplined API governance.

Despite these limitations, the externalized model offers the most sustainable path forward. It preserves CUBE's philosophical integrity as a declarative substrate, provides clients with the procedural interface they require, and positions the ChRIS ecosystem to leverage emerging AI capabilities. The additional operational complexity is a manageable cost for the architectural clarity and evolutionary flexibility gained.

The following Discussion section will evaluate these three approaches—status quo (2.1), embedded (2.3), and externalized (2.4)—in comparative terms, arguing ultimately that the IAS represents the optimal balance between expressive power, operational pragmatism, and long-term adaptability.

